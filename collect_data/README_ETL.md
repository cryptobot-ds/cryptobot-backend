## 1) üì• EXTRACT ‚Äî Sources de donn√©es

Sources utilis√©es (donn√©es publiques uniquement) :

- CoinGecko : prix/historique (api.coingecko.com)

- Binance : prix spot temps r√©el (api.binance.com)

- Fear & Greed Index : sentiment crypto (api.alternative.me)

Les cryptomonnaies suivies :

- Bitcoin (bitcoin / BTC)

- Ethereum (ethereum / ETH)

- Binance Coin (binancecoin / BNB)

---

## 2) üß™ TRANSFORM ‚Äî Indicateurs & features calcul√©es
Indicateurs techniques (calcul√©s dans bot.py)

Exemples :

- RSI

- SMA

- MACD (signal + histogram)

- Bandes de Bollinger (upper/lower)

- (selon impl√©mentation : ADX, Stoch RSI, Fibonacci...)

- Feature engineering (dans generate_features.py)

- Ajouts principaux (upsert en base) :

- volume_avg_7d, volume_avg_14d, volume_avg_30d

- sma_7, sma_14, sma_30

- fear_greed_7d, fear_greed_14d, fear_greed_30d

- change_percent (variation %)

Objectif : produire des variables stables et exploitables pour le ML et l‚Äôanalyse.

---

## 3) üóÉÔ∏è LOAD ‚Äî Stockage PostgreSQL (RDS)

Les donn√©es sont charg√©es dans PostgreSQL (AWS RDS) :

- Table crypto_prices

- prix, volume + indicateurs + features

- upsert via : ON CONFLICT (crypto, timestamp) DO UPDATE

- Table fear_greed_index

- index de sentiment (horodat√©)

Ce m√©canisme garantit :

- pas de doublons

- mise √† jour des features si recalcul√©es

- un stockage ‚Äúpropre‚Äù et coh√©rent pour le ML/API

---
> ‚ÑπÔ∏è **S√©paration ETL / ML**  
> Le module Machine Learning consomme les donn√©es produites par l‚ÄôETL,
> mais ne fait pas partie du pipeline ETL au sens strict.
>  
> L‚ÄôETL a pour objectif de produire des donn√©es fiables et exploitables,
> tandis que le ML constitue une √©tape aval d√©di√©e √† la pr√©diction et √† l‚Äôanalyse.

---

### ‚è±Ô∏è Automatisation (cron)

Le pipeline est con√ßu pour √™tre ex√©cut√© automatiquement :

- Cron (toutes les heures) ‚Üí appelle collect_data/collect_data.sh

Le script orchestre :

- collecte / calculs / insertion

- g√©n√©ration de features

- archivage / nettoyage

- (optionnel) check BDD

- (optionnel) lancement du ML (ml/predict_price.py)

---

### üßπ Archivage & r√©tention

Le script archive_and_clean.py :

archive les fichiers (CSV / logs) dans */archives/

supprime automatiquement les fichiers trop anciens (r√©tention configur√©e, ex : 30 jours)

Objectifs :

- limiter l‚Äôencombrement disque

- conserver une tra√ßabilit√© minimale

- rester frugal en stockage

---

### üìà Monitoring (Node Exporter textfile collector)

Le script collect_data.sh √©crit un fichier de m√©triques Prometheus :

dossier : /var/lib/node_exporter/textfile_collector

fichier : cryptobot_cron.prom

Exemples de m√©triques :

- timestamp du dernier succ√®s

- statut du dernier run (OK / KO)

Cela permet d‚Äôint√©grer la sant√© du pipeline dans Grafana/Prometheus.

---

### üîê S√©curit√© & conformit√©

Donn√©es publiques uniquement (pas de donn√©es personnelles)

Secrets externalis√©s via variables d‚Äôenvironnement (.env / ~/.cryptobot_env)

Acc√®s BDD via compte d√©di√© (RDS)

Pipeline batch (pas de service critique expos√©)

---

### Frugalit√© (co√ªt ma√Ætris√©)

Ex√©cution horaire en batch (pas de streaming continu)

Mod√®le simple et calculs limit√©s

Archivage + r√©tention pour limiter le stockage

R√©utilisation des donn√©es en base (pas de re-collecte inutile)

---

### ‚ñ∂Ô∏è Ex√©cution manuelle (debug)
Pr√©-requis
```
python -m venv .venv
source .venv/bin/activate
pip install -r collect_data/requirements.txt
```
Lancer 
```
cd collect_data
bash collect_data.sh
```

Lancer uniquement les features
python generate_features.py

###  üîé Sorties attendues

- Nouvelles lignes / mises √† jour dans crypto_prices et fear_greed_index

- CSV mis √† jour dans csv/

- logs (si activ√©s)

- m√©triques .prom mises √† jour 


---

## üß© Orchestration (collect_data.sh)

Le script `collect_data.sh` orchestre l‚Äôex√©cution compl√®te du pipeline :
En cas d‚Äô√©chec √† n‚Äôimporte quelle √©tape, le script s‚Äôarr√™te imm√©diatement (set -e) et publie un statut KO via Prometheus.

1. Chargement des variables d‚Äôenvironnement depuis `~/.cryptobot_env`
2. Activation (ou cr√©ation) d‚Äôun environnement virtuel Python `.venv`
3. Installation des d√©pendances : `pip install -r requirements.txt`
4. Ex√©cution :
   - `bot.py` : collecte + calcul indicateurs + insertion BDD
   - `archive_and_clean.py` : archivage + r√©tention CSV/logs
   - `generate_features.py` : feature engineering + upsert
   - `../bdd/check_db.py` (si pr√©sent) : v√©rification basique
   - `../ml/predict_price.py` : pr√©diction + insertion + tracking MLflow
5. Publication m√©triques Node Exporter :
   - succ√®s ‚Üí `cryptobot_cron_last_success_timestamp` + status=1
   - √©chec ‚Üí `cryptobot_cron_last_failure_timestamp` + status=0


## üß≠ Flow (r√©sum√©)

Cron (toutes les heures)  
‚Üí `collect_data/collect_data.sh` (orchestrateur)  
‚Üí `bot.py` + `archive_and_clean.py` + `generate_features.py` + `ml/predict_price.py`  
‚Üí PostgreSQL (RDS) + CSV + logs + m√©triques Prometheus


---

## ‚è±Ô∏è Planification (cron)

Le pipeline est planifi√© avec `cron` sur l‚ÄôEC2 :

```cron
10 * * * * /home/ubuntu/actions-runner/_work/cryptobot-backend/cryptobot-backend/collect_data/collect_data.sh >> /home/ubuntu/actions-runner/_work/cryptobot-backend/cryptobot-backend/logs/collect_data_cron.log 2>&1
```

Ex√©cution : toutes les heures √† HH:10

`Logs : logs/collect_data_cron.log`


## üìà Monitoring (Node Exporter textfile collector)

√Ä la fin de chaque ex√©cution, le script √©crit un fichier de m√©triques Prometheus :

- Dossier : `/var/lib/node_exporter/textfile_collector`
- Fichier : `cryptobot_cron.prom`

M√©triques expos√©es :
- `cryptobot_cron_last_success_timestamp` : timestamp UNIX du dernier succ√®s
- `cryptobot_cron_last_failure_timestamp` : timestamp UNIX du dernier √©chec
- `cryptobot_cron_last_run_status` : 1 (OK) / 0 (KO)

En cas d‚Äôerreur √† n‚Äôimporte quelle √©tape, un `trap` d√©clenche l‚Äô√©criture des m√©triques en √©chec.


## üîÅ Consommation (ML / API / Dashboard)

Les donn√©es stock√©es en base (RDS) alimentent :
- le module ML (`ml/predict_price.py`) ex√©cut√© dans la m√™me orchestration cron,
- l‚ÄôAPI FastAPI (consultation des pr√©dictions / historique),
- le dashboard Streamlit (visualisation).
